{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4cdKPyTQzFPw","executionInfo":{"status":"ok","timestamp":1704573621762,"user_tz":-60,"elapsed":1955,"user":{"displayName":"MIGUEL HABANA","userId":"09775679388784365985"}},"outputId":"debcde0e-9f16-4272-9187-6e406d2ab65e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/Shareddrives/Computational Semantics A3/Code\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/Shareddrives/Computational Semantics A3/Code'"]},{"cell_type":"code","source":["import os\n","import spacy\n","import gensim.downloader as api\n","from gensim.models import Word2Vec\n","import string\n","import numpy as np"],"metadata":{"id":"cNl_eliAzKUJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sentence_segmentation(text, spacy_pipeline:str=\"en_core_web_lg\"):\n","  try:\n","      nlp = spacy.load(spacy_pipeline)\n","  except:\n","      !python3 -m spacy download {spacy_pipeline}\n","      nlp = spacy.load(spacy_pipeline)\n","\n","\n","  doc = nlp(text)\n","  assert doc.has_annotation(\"SENT_START\")\n","  sentences = [sentence.text for sentence in doc.sents]\n","  return sentences\n","\n","def get_relevant_files(decade:int):\n","  files = []\n","  for (dirpath, dirnames, filenames) in os.walk('coha_samples_text'):\n","    # only read the files within the selected decade\n","    files = ['coha_samples_text/'+f for f in filenames if int(f.split(\".\")[0].split(\"_\")[1]) >= decade and int(f.split(\".\")[0].split(\"_\")[1]) <= decade+10]\n","\n","  return files\n","\n","def extract_text(file_path:str):\n","  with open(file_path, 'r', encoding='utf-8') as file:\n","    text = file.read()\n","    translator = str.maketrans('', '', string.punctuation)\n","    text = text.translate(translator)\n","    text = text.lower()\n","\n","  return text\n","\n","def process_text(text:str, output_path:str):\n","  output = \"\"\n","\n","  if os.path.exists(output_path):\n","    return\n","  else:\n","    with open(output_path, 'w') as f:\n","      f.write(output)\n","\n","  if len(text) > 1000000:\n","    sentences = []\n","    for i in range(0, len(text), 1000000):\n","      sentences.extend(sentence_segmentation(text[i:i+1000000]))\n","  else:\n","    sentences = sentence_segmentation(text)\n","\n","  output = \"\\n\".join(sentences)\n","\n","  with open(output_path, 'a') as f:\n","    f.write(output)\n"],"metadata":{"id":"vvrpFYBf9eg-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for decade in range(1860, 2010, 10):\n","  output_path = f\"coha_processed_text/{decade}.txt\"\n","  if not os.path.exists(output_path):\n","    with open(output_path, 'w') as f:\n","      f.write(\"\")\n","\n","  files = get_relevant_files(decade)\n","\n","  for f in files:\n","    text = extract_text(f)\n","    with open(output_path, 'a') as f:\n","      f.write(text)"],"metadata":{"id":"Jssho7Fy3VTd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for decade in range(1860, 2010, 10):\n","  input_path = f\"coha_processed_text/{decade}.txt\"\n","  output_path = f\"coha_processed_text/{decade}_processed.txt\"\n","\n","  text = extract_text(input_path)\n","  process_text(text, output_path)"],"metadata":{"id":"2wr0yfT6513w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","\n","for decade in range(1860, 2010, 10):\n","  model = Word2Vec(min_count=50,max_vocab_size=50000)\n","  model.build_vocab(corpus_file=f'coha_processed_text/{decade}_processed.txt')\n","  total_words = model.corpus_total_words\n","  model.train(corpus_file=f'coha_processed_text/{decade}_processed.txt', total_words=total_words, epochs=5)\n","  model.save(f'coha_decade_models/{decade}.model')"],"metadata":{"id":"AH3jDZcNUmP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZvrxYooWqinC"},"execution_count":null,"outputs":[]}]}