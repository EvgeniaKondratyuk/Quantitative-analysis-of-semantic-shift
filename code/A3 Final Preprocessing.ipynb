{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BIMeuvxhQnyd"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/Shareddrives/Computational Semantics A3/Code'"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import spacy\n","import gensim.downloader as api\n","from gensim.models import FastText\n","import string"],"metadata":{"id":"rRBQLOMsQr3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextProcessor():\n","    def __init__(self, spacy_pipeline:str=\"en_core_web_lg\"):\n","        try:\n","            self.nlp = spacy.load(spacy_pipeline)\n","        except:\n","            !python3 -m spacy download {spacy_pipeline}\n","            self.nlp = spacy.load(spacy_pipeline)\n","\n","    def lemmatize_text(self, text):\n","      doc = self.nlp(text)\n","      lemmas = [token.lemma_ for token in doc]\n","\n","      return \" \".join(lemmas)\n","\n","\n","    def tokenize_text(self, text):\n","        doc = self.nlp(text)\n","        tokens = [token.text for token in doc]\n","        return tokens\n","\n","    def pos_tagging(self, text):\n","        doc = self.nlp(text)\n","        pos = [token.tag_ for token in doc]\n","        return pos\n","\n","    def sentence_segmentation(self, text):\n","        doc = self.nlp(text)\n","        assert doc.has_annotation(\"SENT_START\")\n","        sentences = [sentence.text for sentence in doc.sents]\n","        return sentences"],"metadata":{"id":"run3eyxbQvZH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["files = []\n","\n","for (dirpath, dirnames, filenames) in os.walk('coha_samples_text'):\n","  # only read the files that are before 1910\n","  files = ['coha_samples_text/'+f for f in filenames if int(f.split(\".\")[0].split(\"_\")[1]) <= 1910]\n","\n","coha_corpus = []\n","\n","for f in files:\n","  with open(f, 'r', encoding='utf-8') as file:\n","    text = file.read()\n","    coha_corpus.append(text)"],"metadata":{"id":"O99Vyp47Q6H2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_corpus(folder_path):\n","    corpus = []\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".txt\"):\n","            file_path = os.path.join(folder_path, filename)\n","            ge\n","                corpus.append(text)\n","    return corpus\n","\n","\n","# Load COCA and COHA\n","coca_corpus = load_corpus('coca_samples_text')\n","\n","# Print the first few characters of the first document in each corpus\n","print(\"COCA Corpus:\", coca_corpus[0][:100])\n","print(\"\\nCOHA Corpus:\", coha_corpus[0][:100])"],"metadata":{"id":"egZUHbvTQ8Xv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_processor = TextProcessor()"],"metadata":{"id":"PI_oTgOOQ_4T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# process the corpus so that each line in the text file is one sentence\n","def process_corpus(corpus:list, output_path:str):\n","  output = \"\"\n","  with open(output_path, 'w') as f:\n","    f.write(output)\n","  for text in corpus:\n","    # the maximum number of characters that the spacy sentence segmentation can handle is 1 000 000.\n","    if len(text) > 1000000:\n","      sentences = []\n","      for i in range(0, len(text), 1000000):\n","        sentences.extend(text_processor.sentence_segmentation(text[i:i+1000000]))\n","    else:\n","      sentences = text_processor.sentence_segmentation(text)\n","    output = output + \"\\n\".join(sentences)\n","    with open(output_path, 'a') as f:\n","        f.write(output)"],"metadata":{"id":"_goK07NdRBxR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["process_corpus(coha_corpus, 'coha_corpus.txt')"],"metadata":{"id":"AKCQhOcgRGQF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["process_corpus(coca_corpus, 'coca_corpus.txt')"],"metadata":{"id":"PULdd9tVRISq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modern_model = FastText(window=5)\n","modern_model.build_vocab(corpus_file='coca_corpus.txt')\n","total_words = modern_model.corpus_total_words\n","modern_model.train(corpus_file='coca_corpus.txt', total_words=total_words, epochs=5)"],"metadata":{"id":"mi32DiNgRI6i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modern_model.save('coca_ft.model')"],"metadata":{"id":"WPsDtiakRMBk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["old_model = FastText(window=5)\n","old_model.build_vocab(corpus_file='coha_corpus.txt')\n","total_words = old_model.corpus_total_words\n","old_model.train(corpus_file='coha_corpus.txt', total_words=total_words, epochs=5)"],"metadata":{"id":"uYHCFv2HRN1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["old_model.save('coha_ft.model')"],"metadata":{"id":"niGxDq5ORQXz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Other preprocessing methods that we tried but ultimately did not use:"],"metadata":{"id":"IfKYHnemRSMN"}},{"cell_type":"markdown","source":["## FastText w/ lemmatized, lowercased and punctiations removed from the text"],"metadata":{"id":"vhcgJEK4RWJ5"}},{"cell_type":"code","source":["# process the corpus so that each line in the text file is one sentence\n","def process_corpus_v2(corpus:list, output_path:str):\n","  output = \"\"\n","\n","  with open(output_path, 'w') as f:\n","    f.write(output)\n","  for text in corpus:\n","    # the maximum number of characters that the spacy sentence segmentation can handle is 1 000 000.\n","    if len(text) > 1000000:\n","      sentences = []\n","      for i in range(0, len(text), 1000000):\n","        temp_text = text_processor.lemmatize_text(text[i:i+1000000])\n","        sentences.extend(text_processor.sentence_segmentation(temp_text))\n","    else:\n","      sentences = text_processor.sentence_segmentation(text)\n","    output = output + \"\\n\".join(sentences)\n","    translator = str.maketrans('', '', string.punctuation)\n","    output = output.translate(translator)\n","    with open(output_path, 'a') as f:\n","        f.write(output)"],"metadata":{"id":"tSDVibogRVhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["process_corpus_v2(coha_corpus, 'coha_corpus_v2.txt')"],"metadata":{"id":"vbF_0kCXRdiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["process_corpus_v2(coca_corpus, 'coca_corpus_v2.txt')"],"metadata":{"id":"FgWGywT1Rd6E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modern_model = FastText(window=5)\n","modern_model.build_vocab(corpus_file='coca_corpus_v2.txt')\n","total_words = modern_model.corpus_total_words\n","modern_model.train(corpus_file='coca_corpus_v2.txt', total_words=total_words, epochs=5)"],"metadata":{"id":"RoWUgT-WRfUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modern_model.save('coca_ft_v2.model')"],"metadata":{"id":"HBrMpWxzRhMj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["old_model = FastText(window=5)\n","old_model.build_vocab(corpus_file='coha_corpus_v2.txt')\n","total_words = old_model.corpus_total_words\n","old_model.train(corpus_file='coha_corpus_v2.txt', total_words=total_words, epochs=5)"],"metadata":{"id":"RSgkGlOxRjp-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["old_model.save('coha_ft_v2.model')"],"metadata":{"id":"qa_AhnC6Rk5p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Word2Vec original Hamilton et al implementation"],"metadata":{"id":"dT-6BUAYRmVu"}},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","\n","old_model = Word2Vec(min_count=100,max_vocab_size=50000)\n","old_model.build_vocab(corpus_file='coha_corpus.txt')\n","total_words = old_model.corpus_total_words\n","old_model.train(corpus_file='coha_corpus.txt', total_words=total_words, epochs=5)"],"metadata":{"id":"uznTU07oRldM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["old_model.save(\"coha_w2v.model\")"],"metadata":{"id":"n7k57__hRpCt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modern_model = Word2Vec(min_count=50,max_vocab_size=50000)\n","modern_model.build_vocab(corpus_file='coca_corpus.txt')\n","total_words = modern_model.corpus_total_words\n","modern_model.train(corpus_file='coca_corpus.txt', total_words=total_words, epochs=5)"],"metadata":{"id":"m63phrEaR18H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modern_model.save(\"coca_w2v.model\")"],"metadata":{"id":"aA_rjBSmR3bW"},"execution_count":null,"outputs":[]}]}